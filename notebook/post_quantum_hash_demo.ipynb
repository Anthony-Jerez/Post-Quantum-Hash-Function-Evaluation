{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089d9599",
   "metadata": {},
   "source": [
    "# Post‑Quantum Hash Function ‑ Demo Notebook\n",
    "This notebook reproduces the toy example, data generation, preprocessing, and LSTM‑based pre‑image attack that our team constructed.\n",
    "\n",
    "Based on the research paper *“Post‑quantum hash functions using $\\mathrm{SL}_n(\\mathbb F_p)$”* written by Corentin Le Coz, Christopher Battarbee, Ramon Flores, Thomas Koberda, and Delaram Kahrobaei."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa6dec",
   "metadata": {},
   "source": [
    "## 1. Constructing the toy hash instance\n",
    "We follow the concrete parameters from Section 2.4 of the paper: $n=3$, $a=4$, $b=2$, walk length $\\ell=4$ and prime $p=5$.\n",
    "The helper functions build the matrices $A$ and $B$, raise them to power $\\ell$, take inverses, and map the IDs 1, 2, 3 as described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74deffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.94190977e+08  2.33260720e+08  2.92979520e+07]\n",
      " [-3.83796480e+07 -1.28962550e+07 -1.61979200e+06]\n",
      " [ 1.19193600e+06  4.00512000e+05  5.03050000e+04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_matrices(n, a, b):\n",
    "    A = np.eye(n)\n",
    "    B = np.eye(n)\n",
    "    for i in range(n-1):\n",
    "        A[i, i+1] = a\n",
    "        B[i+1, i] = b\n",
    "    return A, B\n",
    "\n",
    "def matrix_power(M, l): \n",
    "    return np.linalg.matrix_power(M, l)\n",
    "\n",
    "def inverse_matrix(M):  \n",
    "    return np.linalg.inv(M)\n",
    "\n",
    "def compute_hash(sequence, mapping):\n",
    "    result = np.eye(mapping[1].shape[0])\n",
    "    for number in sequence:\n",
    "        result = np.dot(result, mapping[number])\n",
    "    return result\n",
    "\n",
    "# toy parameters\n",
    "n,a,b,l = 3,4,2,4\n",
    "A,B = create_matrices(n,a,b)\n",
    "A_l, B_l = matrix_power(A,l), matrix_power(B,l)\n",
    "# create the mapping\n",
    "A_inv, B_inv = inverse_matrix(A_l), inverse_matrix(B_l)\n",
    "mapping = {\n",
    "    1: B_l,\n",
    "    2: A_inv,\n",
    "    3: B_inv\n",
    "}\n",
    "sequence = [2,2,3,2,2,2,1]\n",
    "result = compute_hash(sequence, mapping)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdadacd8",
   "metadata": {},
   "source": [
    "> The printed matrix matches the concrete example in the paper, confirming the construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78b0b0",
   "metadata": {},
   "source": [
    "## 2. Generating all sequences of length 8 and their hashes\n",
    "We enumerate all $3^8$ sequences over the alphabet {1,2,3}, compute the hash mod $p=5$, and store flattened outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1970a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 6,561 input–output pairs.\n"
     ]
    }
   ],
   "source": [
    "import itertools, numpy as np, torch\n",
    "p = 5\n",
    "\n",
    "def hash_mod(seq_str):\n",
    "    seq_ints = [int(ch) for ch in seq_str]\n",
    "    return np.mod(compute_hash(seq_ints, mapping), p)\n",
    "\n",
    "seqs = [''.join(s) for s in itertools.product('123', repeat=8)]\n",
    "inputs = np.array(seqs)\n",
    "outputs = np.array([hash_mod(s).flatten() for s in seqs])\n",
    "\n",
    "np.save('all_inputs.npy', inputs)\n",
    "np.save('all_outputs.npy', outputs)\n",
    "print(f\"Saved {len(seqs):,} input–output pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b6674",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "Convert character sequences to integer tokens and build a `TensorDataset` for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19523d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDataset saved with shapes torch.Size([6561, 8]) torch.Size([6561, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "token_inputs = [[int(ch) for ch in seq] for seq in inputs]\n",
    "X = torch.tensor(token_inputs, dtype=torch.long)\n",
    "y = torch.tensor(outputs, dtype=torch.float32)\n",
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "torch.save(dataset, 'dataset.pth')\n",
    "print('TensorDataset saved with shapes', X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0f3f7",
   "metadata": {},
   "source": [
    "## 4. LSTM model and training\n",
    "We use a single‑layer LSTM (hidden size 100) followed by a linear head, trained with MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c5faf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3281, Probe size: 3280, Total: 6561\n",
      "Epoch 01 train MSE 2.4545\n",
      "Epoch 02 train MSE 1.9083\n",
      "Epoch 03 train MSE 1.8967\n",
      "Epoch 04 train MSE 1.8729\n",
      "Epoch 05 train MSE 1.8231\n",
      "Epoch 06 train MSE 1.7684\n",
      "Epoch 07 train MSE 1.7436\n",
      "Epoch 08 train MSE 1.7335\n",
      "Epoch 09 train MSE 1.7261\n",
      "Epoch 10 train MSE 1.7161\n",
      "Epoch 11 train MSE 1.7020\n",
      "Epoch 12 train MSE 1.6644\n",
      "Epoch 13 train MSE 1.6241\n",
      "Epoch 14 train MSE 1.5787\n",
      "Epoch 15 train MSE 1.5373\n",
      "Epoch 16 train MSE 1.4915\n",
      "Epoch 17 train MSE 1.4509\n",
      "Epoch 18 train MSE 1.4048\n",
      "Epoch 19 train MSE 1.3643\n",
      "Epoch 20 train MSE 1.3200\n",
      "Epoch 21 train MSE 1.2805\n",
      "Epoch 22 train MSE 1.2371\n",
      "Epoch 23 train MSE 1.2016\n",
      "Epoch 24 train MSE 1.1636\n",
      "Epoch 25 train MSE 1.1319\n",
      "Epoch 26 train MSE 1.0968\n",
      "Epoch 27 train MSE 1.0710\n",
      "Epoch 28 train MSE 1.0371\n",
      "Epoch 29 train MSE 1.0093\n",
      "Epoch 30 train MSE 0.9803\n",
      "Epoch 31 train MSE 0.9525\n",
      "Epoch 32 train MSE 0.9252\n",
      "Epoch 33 train MSE 0.8988\n",
      "Epoch 34 train MSE 0.8760\n",
      "Epoch 35 train MSE 0.8527\n",
      "Epoch 36 train MSE 0.8276\n",
      "Epoch 37 train MSE 0.8041\n",
      "Epoch 38 train MSE 0.7840\n",
      "Epoch 39 train MSE 0.7613\n",
      "Epoch 40 train MSE 0.7443\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Load full dataset and unpack tensors\n",
    "dataset = torch.load('dataset.pth', weights_only=False)\n",
    "X_all, y = dataset.tensors\n",
    "\n",
    "# Train: 50%, Probe: 50%\n",
    "probe_frac = 0.50\n",
    "probe_sz = int(len(dataset) * probe_frac)\n",
    "train_sz = len(dataset) - probe_sz\n",
    "train_ds, probe_ds = random_split(dataset, [train_sz, probe_sz], generator=torch.Generator().manual_seed(42))\n",
    "print(f\"Train size: {train_sz}, Probe size: {probe_sz}, Total: {len(dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "probe_loader = DataLoader(probe_ds, batch_size=32)\n",
    "\n",
    "# Define the LSTM model\n",
    "class HashPredictor(nn.Module):\n",
    "    def __init__(self, vocab=4, embed=32, hidden=100, out_dim=None):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab, embed)\n",
    "        self.lstm = nn.LSTM(embed, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        return self.fc(h.squeeze(0))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HashPredictor(out_dim=y.shape[1]).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(40):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = loss_fn(model(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.item() * xb.size(0)\n",
    "    print(f'Epoch {epoch+1:02d} train MSE {epoch_loss/train_sz:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c83b598",
   "metadata": {},
   "source": [
    "## 5. Evaluating search‑space reduction\n",
    "Rank only the probe candidates and compute the average fraction of the space the attacker can skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f27fa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe set MSE: 0.9053\n",
      "\n",
      "Average search-space reduction on unseen probe set: 93.88%\n"
     ]
    }
   ],
   "source": [
    "probe_inputs, probe_hashes = probe_ds[:]\n",
    "probe_inputs, probe_hashes = probe_inputs.to(device), probe_hashes.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_preds = model(X_all.to(device))\n",
    "    probe_preds = model(probe_inputs)\n",
    "    mse_probe = nn.functional.mse_loss(probe_preds, probe_hashes)\n",
    "    \n",
    "print(f\"Probe set MSE: {mse_probe.item():.4f}\")\n",
    "\n",
    "reductions = []\n",
    "for t_input, t_hash in zip(probe_inputs, probe_hashes):\n",
    "    t_input = t_input.unsqueeze(0)\n",
    "    t_hash = t_hash.unsqueeze(0)\n",
    "    dists = torch.norm(all_preds - t_hash, dim=1)\n",
    "    true_index = (X_all == t_input.squeeze(0)).all(dim=1).nonzero(as_tuple=False).item()\n",
    "    rank = torch.argsort(dists).tolist().index(true_index) + 1\n",
    "    reductions.append(1 - rank / len(X_all))\n",
    "\n",
    "print(f'\\nAverage search-space reduction on unseen probe set: {100 * torch.tensor(reductions).mean():.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.9 ('csci381')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "98d1ce12ff3c938e4543b64a07aed4b1eb744abb365e76568f2762e671738769"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
